{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam trening XGBoost...\n",
      "--- 5. Model XGBoost został wytrenowany ---\n",
      "Dokładność: 0.9567\n",
      "Raport klasyfikacji:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Odrzucony (0)       0.99      0.95      0.97       244\n",
      "Zaproszony (1)       0.83      0.96      0.89        56\n",
      "\n",
      "      accuracy                           0.96       300\n",
      "     macro avg       0.91      0.96      0.93       300\n",
      "  weighted avg       0.96      0.96      0.96       300\n",
      "\n",
      "Dokładność na zbiorze TRENINGOWYM: 0.9829\n",
      "Dokładność na zbiorze TESTOWYM:    0.9567\n",
      "\n",
      "Top 10 najważniejszych cech wg XGBoost:\n",
      "                     Cecha   Ważność\n",
      "0       Experience (Years)  0.397817\n",
      "5           Projects Count  0.178343\n",
      "3                 Job Role  0.032607\n",
      "29                  Python  0.030143\n",
      "22           Deep Learning  0.028299\n",
      "8            Deep Learning  0.027577\n",
      "15                  Python  0.024883\n",
      "4   Salary Expectation ($)  0.023952\n",
      "30                 Pytorch  0.023309\n",
      "19              TensorFlow  0.021223\n",
      "\n",
      "Uruchamiam 5-krotną walidację krzyżową...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianstarzec/miniconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [20:36:34] WARNING: /var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_d9k8pmaj4_/croot/xgboost-split_1724073758172/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__sklearn_tags__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(feature_importance_df.head(\u001b[32m10\u001b[39m))\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUruchamiam 5-krotną walidację krzyżową...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m cv_scores = cross_val_score(model, X, y, cv=\u001b[32m5\u001b[39m)\n\u001b[32m     74\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWyniki poszczególnych prób CV: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mŚredni wynik CV: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores.mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (+/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores.std()\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m2\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:684\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    682\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m cv_results = cross_validate(\n\u001b[32m    685\u001b[39m     estimator=estimator,\n\u001b[32m    686\u001b[39m     X=X,\n\u001b[32m    687\u001b[39m     y=y,\n\u001b[32m    688\u001b[39m     groups=groups,\n\u001b[32m    689\u001b[39m     scoring={\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m: scorer},\n\u001b[32m    690\u001b[39m     cv=cv,\n\u001b[32m    691\u001b[39m     n_jobs=n_jobs,\n\u001b[32m    692\u001b[39m     verbose=verbose,\n\u001b[32m    693\u001b[39m     params=params,\n\u001b[32m    694\u001b[39m     pre_dispatch=pre_dispatch,\n\u001b[32m    695\u001b[39m     error_score=error_score,\n\u001b[32m    696\u001b[39m )\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:347\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    345\u001b[39m X, y = indexable(X, y)\n\u001b[32m    346\u001b[39m params = {} \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m params\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m cv = check_cv(cv, y, classifier=is_classifier(estimator))\n\u001b[32m    349\u001b[39m scorers = check_scoring(\n\u001b[32m    350\u001b[39m     estimator, scoring=scoring, raise_exc=(error_score == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    351\u001b[39m )\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n\u001b[32m    354\u001b[39m     \u001b[38;5;66;03m# For estimators, a MetadataRouter is created in get_metadata_routing\u001b[39;00m\n\u001b[32m    355\u001b[39m     \u001b[38;5;66;03m# methods. For these router methods, we create the router to use\u001b[39;00m\n\u001b[32m    356\u001b[39m     \u001b[38;5;66;03m# `process_routing` on it.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/sklearn/base.py:1237\u001b[39m, in \u001b[36mis_classifier\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m   1230\u001b[39m     warnings.warn(\n\u001b[32m   1231\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect.stack()[\u001b[32m0\u001b[39m][\u001b[32m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1232\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1233\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1234\u001b[39m     )\n\u001b[32m   1235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m_estimator_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m get_tags(estimator).estimator_type == \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/_tags.py:430\u001b[39m, in \u001b[36mget_tags\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator).mro()):\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m__sklearn_tags__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m         sklearn_tags_provider[klass] = klass.__sklearn_tags__(estimator)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    431\u001b[39m         class_order.append(klass)\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_more_tags\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/sklearn/base.py:540\u001b[39m, in \u001b[36mClassifierMixin.__sklearn_tags__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     tags = \u001b[38;5;28msuper\u001b[39m().__sklearn_tags__()\n\u001b[32m    541\u001b[39m     tags.estimator_type = \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    542\u001b[39m     tags.classifier_tags = ClassifierTags()\n",
      "\u001b[31mAttributeError\u001b[39m: 'super' object has no attribute '__sklearn_tags__'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "FILE_PATH = 'data_after_filter.csv'\n",
    "MODEL_FILE_NAME = 'model_xgboost.pkl'\n",
    "RESULTS_FILE_NAME = 'raport_xgboost.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"BŁĄD: Nie znaleziono pliku: {FILE_PATH}\")\n",
    "    exit()\n",
    "\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "X = df.drop(['Resume_ID', 'Recruiter Decision'], axis=1, errors='ignore')\n",
    "y = df['Recruiter Decision']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y \n",
    ")\n",
    "\n",
    "print(\"Rozpoczynam trening XGBoost...\")\n",
    "ratio = float(np.sum(y == 0)) / np.sum(y == 1)\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,       # Liczba drzew\n",
    "    learning_rate=0.1,      # Szybkość uczenia (mniejsza = dokładniej, ale wolniej)\n",
    "    max_depth=3,            # Głębokość drzewa (zapobiega przeuczeniu)\n",
    "    scale_pos_weight=ratio, # Balansowanie klas (kluczowe w rekrutacji!)\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"--- 5. Model XGBoost został wytrenowany ---\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=['Odrzucony (0)', 'Zaproszony (1)'])\n",
    "\n",
    "print(f\"Dokładność: {accuracy:.4f}\")\n",
    "print(\"Raport klasyfikacji:\\n\", report)\n",
    "\n",
    "\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Dokładność na zbiorze TRENINGOWYM: {train_accuracy:.4f}\")\n",
    "print(f\"Dokładność na zbiorze TESTOWYM:    {test_accuracy:.4f}\")\n",
    "\n",
    "# --- 7. Ważność Cech ---\n",
    "importances = model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {'Cecha': X.columns, 'Ważność': importances}\n",
    ").sort_values(by='Ważność', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 najważniejszych cech wg XGBoost:\")\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "print(f\"Wyniki poszczególnych prób CV: {cv_scores}\")\n",
    "print(f\"Średni wynik CV: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# --- 8. Zapis wyników ---\n",
    "with open(RESULTS_FILE_NAME, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"Model: XGBoost\\n\")\n",
    "    f.write(f\"Dokładność: {accuracy:.4f}\\n\\n\")\n",
    "    f.write(\"Raport:\\n\")\n",
    "    f.write(report)\n",
    "    f.write(\"\\n\\nWażność cech:\\n\")\n",
    "    f.write(feature_importance_df.head(10).to_string())\n",
    "\n",
    "joblib.dump(model, MODEL_FILE_NAME)\n",
    "print(f\"\\nZapisano model do {MODEL_FILE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam proces trenowania i walidacji...\n",
      "Wyniki poszczególnych prób CV: [0.955 0.97  0.96  0.945 0.945]\n",
      "Średni wynik CV: 0.9550\n",
      "Stabilność (Odchylenie stand.): +/- 0.0095\n",
      "Dokładność na zbiorze TESTOWYM: 0.9567\n",
      "Raport klasyfikacji:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Odrzucony (0)       0.99      0.95      0.97       244\n",
      "Zaproszony (1)       0.83      0.96      0.89        56\n",
      "\n",
      "      accuracy                           0.96       300\n",
      "     macro avg       0.91      0.96      0.93       300\n",
      "  weighted avg       0.96      0.96      0.96       300\n",
      "\n",
      "Dokładność Treningowa: 0.9829\n",
      "Dokładność Testowa:    0.9567\n",
      "Różnica (Gap):         0.0262\n",
      "\n",
      "Top 10 najważniejszych cech wg XGBoost:\n",
      "                     Cecha   Ważność\n",
      "0       Experience (Years)  0.397817\n",
      "5           Projects Count  0.178343\n",
      "3                 Job Role  0.032607\n",
      "29                  Python  0.030143\n",
      "22           Deep Learning  0.028299\n",
      "8            Deep Learning  0.027577\n",
      "15                  Python  0.024883\n",
      "4   Salary Expectation ($)  0.023952\n",
      "30                 Pytorch  0.023309\n",
      "19              TensorFlow  0.021223\n",
      "\n",
      "Model zapisany pomyślnie jako: model_xgboost.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- KONFIGURACJA ---\n",
    "FILE_PATH = 'data_after_filter.csv'\n",
    "MODEL_FILE_NAME = 'model_xgboost.pkl'\n",
    "RESULTS_FILE_NAME = 'raport_xgboost.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"BŁĄD: Nie znaleziono pliku: {FILE_PATH}\")\n",
    "    exit()\n",
    "\n",
    "# Usuwanie duplikatów kolumn\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "# Przygotowanie danych (X - cechy, y - cel)\n",
    "X = df.drop(['Resume_ID', 'Recruiter Decision'], axis=1, errors='ignore')\n",
    "y = df['Recruiter Decision']\n",
    "\n",
    "# Podział na zbiór treningowy i testowy (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y \n",
    ")\n",
    "\n",
    "print(\"Rozpoczynam proces trenowania i walidacji...\")\n",
    "\n",
    "# Obliczanie balansu klas dla parametru scale_pos_weight\n",
    "ratio = float(np.sum(y == 0)) / np.sum(y == 1)\n",
    "\n",
    "# --- PUNKT 5: TRENOWANIE I WALIDACJA ---\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for train_index, val_index in skf.split(X, y):\n",
    "    # Podział na foldy\n",
    "    X_train_cv, X_val_cv = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_cv, y_val_cv = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Model z ograniczoną głębokością (max_depth=3) przeciw przeuczeniu\n",
    "    cv_model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        scale_pos_weight=ratio,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    cv_model.fit(X_train_cv, y_train_cv)\n",
    "    score = cv_model.score(X_val_cv, y_val_cv)\n",
    "    cv_scores.append(score)\n",
    "\n",
    "cv_scores = np.array(cv_scores)\n",
    "print(f\"Wyniki poszczególnych prób CV: {cv_scores}\")\n",
    "print(f\"Średni wynik CV: {cv_scores.mean():.4f}\")\n",
    "print(f\"Stabilność (Odchylenie stand.): +/- {cv_scores.std():.4f}\")\n",
    "\n",
    "# 2. Trening modelu końcowego\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    scale_pos_weight=ratio,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Ocena na zbiorze testowym\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=['Odrzucony (0)', 'Zaproszony (1)'])\n",
    "\n",
    "print(f\"Dokładność na zbiorze TESTOWYM: {accuracy:.4f}\")\n",
    "print(\"Raport klasyfikacji:\\n\", report)\n",
    "\n",
    "# Porównanie wyników (diagnoza overfittingu)\n",
    "train_acc = model.score(X_train, y_train)\n",
    "print(f\"Dokładność Treningowa: {train_acc:.4f}\")\n",
    "print(f\"Dokładność Testowa:    {accuracy:.4f}\")\n",
    "print(f\"Różnica (Gap):         {abs(train_acc - accuracy):.4f}\")\n",
    "\n",
    "# --- PUNKT 6: IMPLEMENTACJA (Ważność cech i zapis) ---\n",
    "\n",
    "importances = model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {'Cecha': X.columns, 'Ważność': importances}\n",
    ").sort_values(by='Ważność', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 najważniejszych cech wg XGBoost:\")\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "# Zapis wyników do pliku tekstowego\n",
    "with open(RESULTS_FILE_NAME, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"Sredni wynik Cross-Validation: {cv_scores.mean():.4f}\\n\")\n",
    "    f.write(f\"Odchylenie standardowe CV: {cv_scores.std():.4f}\\n\")\n",
    "    f.write(f\"Dokladnosc na zbiorze testowym: {accuracy:.4f}\\n\\n\")\n",
    "    f.write(\"Raport klasyfikacji:\\n\")\n",
    "    f.write(report)\n",
    "    f.write(\"\\n\\nNajwazniejsze cechy (Top 10):\\n\")\n",
    "    f.write(feature_importance_df.head(10).to_string())\n",
    "\n",
    "# Zapis modelu do pliku .pkl\n",
    "joblib.dump(model, MODEL_FILE_NAME)\n",
    "print(f\"\\nModel zapisany pomyślnie jako: {MODEL_FILE_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
